{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "270a3765-3c17-408b-83bc-e15536d796f6",
   "metadata": {},
   "source": [
    "The goal of the following hands-on excercises is for you to become familiar with basic deep learning concepts.\n",
    "\n",
    "At the end of the exercises, you should have an understanding of the following ideas:\n",
    "* multi-layer perceptron (feed-foward neural net)\n",
    "* loss functions (regression and classification)\n",
    "* different optimization algorithms\n",
    "* how to read loss curves\n",
    "* how to tune hyper-parameters.\n",
    "\n",
    "Along the way, we will introduce some basic syntax for the `PyTorch` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9baf280-ab19-4a23-86db-436a4f612ca9",
   "metadata": {},
   "source": [
    "Import some modules we'll use sooner or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb649e-f066-4d7a-a7b7-b1ac228bdf19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# progress bars are really nice, use them!\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c77af2-c567-4ab8-8b6e-669b18856870",
   "metadata": {},
   "source": [
    "# 1. Defining the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362853e8-e2c7-4b63-8260-5c695e2a7642",
   "metadata": {},
   "source": [
    "## Primitive building block: the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6638a2-056f-495b-b9b0-96826e812a3d",
   "metadata": {},
   "source": [
    "We first construct a **Perceptron**.\n",
    "A perceptron is the fundamental building block of many neural networks.\n",
    "It performs the following operations on the input `x` sequentially:\n",
    "1. pass through an affine transformation (`nn.Linear`), which consists of multiplying by a weight and adding a bias.\n",
    "2. pass through a non-linear activation function. The most popular choice is the rectifying linear unit (`nn.ReLU`).\n",
    "\n",
    "In `torch`, we need to \"register\" these operations in the constructor (`__init__`).\n",
    "This tells `torch` to track gradients with respect to parameters associated with the operations.\n",
    "Technically speaking, most activation functions (like `nn.ReLU`) don't have trainable parameters.\n",
    "However, it is still good practice to register them as modules in the constructor.\n",
    "\n",
    "Tip: the [PyTorch online documentation](https://pytorch.org/docs/stable/index.html) is very well written.\n",
    "You'll find it useful for the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234daa8-aca4-4d94-a3b7-5bb6d2d7f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron (nn.Module) :\n",
    "    # Unless we have a more specialized unit, we inherit from nn.Module.\n",
    "    # This base class performs a lot of magic under the hood to make it easy\n",
    "    # for us to write neural nets.\n",
    "\n",
    "    def __init__ (\n",
    "            self,\n",
    "            dim_in: int, # input dimensionality\n",
    "            dim_out: int # output dimensionality\n",
    "    ) :\n",
    "        # usually, we need to initialize the base class first\n",
    "        # The torch Module performs some magic when instance members are added\n",
    "        super().__init__()\n",
    "        self.l = ...\n",
    "        self.activation = ...\n",
    "\n",
    "    def forward (self, x: torch.Tensor) -> torch.Tensor :\n",
    "        # torch wraps forward(...) within the __call__ method\n",
    "        # and takes care of tracing gradients automatically\n",
    "        return self.activation(self.l(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893c295-2b93-4e98-8aaa-5cadc57ab114",
   "metadata": {},
   "source": [
    "## Feed-foward neural net: multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24372e44-7dc9-4744-a284-987648b38d17",
   "metadata": {},
   "source": [
    "Next, we stack multiple perceptrons into a **Multi-Layer Perceptron (MLP)**.\n",
    "\n",
    "Each perceptron's output is counted as a *hidden layer*.\n",
    "The dimensionality of the hidden layers, `dim_hidden`, is a hyperparameter and not constrained\n",
    "by the input or output dimensionality.\n",
    "\n",
    "Hints:\n",
    "1. the output of your MLP should in principle cover the entire space.\\\n",
    "   The output of activation functions like ReLU does not, in general.\n",
    "2. You'll need to be a bit careful when registering lists of perceptrons in the constructor.\\\n",
    "   PyTorch doesn't know that an ordinary list contains modules it should care about.\\\n",
    "   Look at the documentation for `nn.Sequential` or `nn.ModuleList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3e2eb-a825-4959-ba46-b7d1d4c0f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron (nn.Module) :\n",
    "\n",
    "    def __init__ (\n",
    "        self,\n",
    "        dim_in: int, # input dimensionality\n",
    "        dim_out: int, # output dimensionality\n",
    "        N_hidden: int, # number of hidden layers\n",
    "        dim_hidden: int, # dimensionality of hidden layers\n",
    "    ) :\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward (self, x: torch.Tensor) -> torch.Tensor :\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42cbfe8-48f0-4e24-8af0-a9d715a5bdf7",
   "metadata": {},
   "source": [
    "Let's make sure that everything is set up correctly. The following should execute without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15570f5-10b7-4fec-8488-f92c8604d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dim_in = 4\n",
    "test_dim_out = 3\n",
    "test_N_hidden = 5\n",
    "test_dim_hidden = 6\n",
    "test_batch_size = 32\n",
    "\n",
    "test_model = MultiLayerPerceptron(test_dim_in, test_dim_out, test_N_hidden, test_dim_hidden)\n",
    "test_x = torch.ones(test_batch_size, test_dim_in)\n",
    "test_y = test_model(test_x)\n",
    "test_loss = test_y.sum()\n",
    "test_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef4e28-d466-4785-8ee3-2e338b8a78c7",
   "metadata": {},
   "source": [
    "# 2. Regression in one dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81800240-4752-4d1c-97f8-be3208a4af4a",
   "metadata": {},
   "source": [
    "The first exercise will be to learn a simple one-dimesional function.\n",
    "We consider the function $f(x) = \\sin(\\pi x)$ on the domain $x \\in [-1, 1]$.\n",
    "\n",
    "Note that even in this simple example we need to be very careful about specifying the domain!\n",
    "Neural networks are generally not very good at out-of-domain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae69f0c-fd28-41e3-a277-7ab00531b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_fct (x: torch.Tensor) -> torch.Tensor :\n",
    "    return (torch.pi * x).sin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67385ef1-3304-4919-8b91-16fc9b26fc36",
   "metadata": {},
   "source": [
    "## Model and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed3ecb-c05d-4068-b012-9a0d28afba24",
   "metadata": {},
   "source": [
    "Let's construct an MLP to learn this function.\n",
    "\n",
    "We'll start with a single hidden layer. What kind of model would our network be equivalent to if there were no hidden layer at all?\n",
    "\n",
    "Let's start with a single hidden unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cafca-f21b-4552-ac4f-4274d198f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = MultiLayerPerceptron(dim_in=1, dim_out=1, N_hidden=1, dim_hidden=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae24a0-b791-45c2-a0fd-9e7c28a6bd3d",
   "metadata": {},
   "source": [
    "In order to train this model, we need to specify a **loss function**.\n",
    "\n",
    "The loss function depends on the kind of problem you want to solve.\n",
    "\n",
    "In this case, it is a *regression problem* (we're trying to fit a function $f(x)$).\n",
    "A popular choice for regression is the mean squared error loss (MSE), defined as\n",
    "$l_{MSE}(y_1, y_2) = \\frac{1}{N}\\sum(y_1 - y_2)^2$,\n",
    "element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914331c-4a69-4093-9eec-d087d7c5fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352cfda-38b0-4b48-b6d7-8d879f43bb8e",
   "metadata": {},
   "source": [
    "## Preliminaries for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0209983-dc5e-4612-b8a9-8cdfec1be383",
   "metadata": {},
   "source": [
    "Let's write down the basic set of hyper parameters for the training. You can play with these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de10684-4fc9-46e0-a779-3a6786ec0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "batch_size = 128\n",
    "N_steps = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0f8c0-a01b-4310-948e-e6acd2f9f675",
   "metadata": {},
   "source": [
    "`PyTorch` provides optimizers out of the box in the `torch.optim` module.\n",
    "\n",
    "Let's start with the simplest one, stochastic gradient descent without momentum or any other bells and whistles (`torch.optim.SGD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5fadd-c888-4626-b82b-0d7eb11591f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(regressor.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2600b-7444-4930-b623-cf06eda6f363",
   "metadata": {},
   "source": [
    "Some general advice: **random numbers are tricky!**\n",
    "\n",
    "Bugs related to random number generation are notoriously hard to find.\n",
    "Many researchers will have some story about days and weeks of frustrating debugging.\n",
    "\n",
    "At the very least, you should use an explicit generator and seed.\n",
    "Working with implicit, global generators is **strongly discouraged**.\n",
    "\n",
    "These things become especially important when your projects require multi-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef365a05-1a40-42f3-90a4-14cfaa72a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = torch.Generator()\n",
    "rng.manual_seed(137)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7895ea-e25e-491c-874f-bbac3a85aba8",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9459fd88-3bb2-4b28-95ba-4dd6f99f3a62",
   "metadata": {},
   "source": [
    "Now we're ready to run our training loop. Usually, the following steps are performed on each iteration:\n",
    "1. fetch a mini-batch of training examples. In actual research, this will come from measurements or simulations. In our simple case, we can just evaluate our `target_fct`.\n",
    "2. zero out any existing gradients (`optimizer.zero_grad()`).\n",
    "3. pass the training example through the model.\n",
    "4. compute the loss (using our previously constructed `loss_fct`).\n",
    "5. back-propagate gradients from the loss (`backward()` method).\n",
    "6. update model parameters using the computed gradients (`optimizer.step()`)\n",
    "\n",
    "It's good practice to keep track of the training. We like the solution provided by [tensorboard](https://pytorch.org/docs/stable/tensorboard.html).\n",
    "In our case, this is overkill as the training runs super fast.\n",
    "We simply append the training loss to a list and inspect after the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb9197-01e2-4886-99c0-8d484b3061d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "\n",
    "for ii in tqdm.tqdm(range(N_steps)) :\n",
    "    x = 2 * torch.rand(batch_size, 1, generator=rng) - 1\n",
    "    y = target_fct(x)\n",
    "\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    train_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e498b2-286f-4ef5-ad48-ffe2d69c276e",
   "metadata": {},
   "source": [
    "Let's look at how our training went:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a703289-b245-4b2a-bce2-0e99e1efd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(np.array(train_losses))\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcdcbc6-7414-4916-a63e-9b26787ee8a6",
   "metadata": {},
   "source": [
    "Does this look as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124a340-dc1f-4341-912f-328d97c076cb",
   "metadata": {},
   "source": [
    "## Inspecting the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b80a9-11a2-4f64-abc4-6d35fe61d7fb",
   "metadata": {},
   "source": [
    "In this simple one-dimensional case, we can also just evaluate the model on some test data.\n",
    "\n",
    "The `torch.no_grad` context manager automatically stops tracing gradients.\n",
    "This is useful to improve efficiency during validation, testing, and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06e5d7-1de1-4037-a462-caf05a14cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(-1, 1, 256)\n",
    "y_test = target_fct(x_test)\n",
    "\n",
    "with torch.no_grad() :\n",
    "    ypred_test = regressor(x_test.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede17482-66bc-4a46-83d0-e9dde3c7811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x_test, y_test, label='ground truth, $f(x) = \\sin(\\pi x)$')\n",
    "plt.plot(x_test, ypred_test, label='model prediction')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f925fc4-0db0-4a2a-bb09-585a40de9a42",
   "metadata": {},
   "source": [
    "Now, go back and figure out what went wrong.\n",
    "\n",
    "Please keep `N_hidden` fixed for easier interpretation.\n",
    "What do you observe as you increase `dim_hidden`?\n",
    "\n",
    "Vanilla SGD is not a good optimizer. Try Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e84a7-c576-4538-bc59-a3a601fdc424",
   "metadata": {},
   "source": [
    "## Bonus task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1aaae-efef-4b20-a0e2-296b822e1866",
   "metadata": {},
   "source": [
    "If you made it here and would like something to think about:\n",
    "So far, we have worked with the domain $x \\in [-1, 1]$.\n",
    "Let's shift this domain to $x \\in [10^3-1, 10^3+1]$.\n",
    "Repeat the above steps with this new domain.\n",
    "What do you find? If there are problems, how could you solve them?\n",
    "\n",
    "(Note that $10^3 \\pm 1$ is still quite accurately representable by 32-bit floats, which PyTorch uses by default.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431499e-120a-4840-b9bb-4c8ed0aef687",
   "metadata": {},
   "source": [
    "# 3. Classification of hand-written digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2f9a3-8355-4892-b559-882108955063",
   "metadata": {},
   "source": [
    "So far, we have only looked at regression. The other big class of tasks is classification.\n",
    "\n",
    "A classifier maps inputs to discrete classes.\n",
    "\n",
    "Since the target space is discrete, we'll need to make some adjustments.\n",
    "If there are $C$ classes, we'll let the output space of our model be $R^C$.\n",
    "This way, we can interpret the $i$th element of the output vector as the (un-normalized) log-probability of the $i$th class.\n",
    "Why is it convenient to work with log-probability instead of probability?\n",
    "\n",
    "A popular loss function to optimize is the cross entropy (`nn.CrossEntropyLoss`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0e656-34ab-405a-9b05-2d308338f231",
   "metadata": {},
   "source": [
    "In this exercise, we'll classify images of grey-scale hand-written digits (MNIST data set).\n",
    "These images have $28 \\times 28$ pixels\n",
    "There are 10 classes (digits 0 through 9).\n",
    "Using the `MultiLayerPerceptron` you wrote before, construct a classifier for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab09a3-5118-4526-8b5c-8e569e65fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84758159-31a2-4375-ad8f-7061b57fba9d",
   "metadata": {},
   "source": [
    "Our loss function is the cross entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b920f-1a44-4ffb-8650-36a0fa0b5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb45e7e-4694-46b4-91eb-49cc9bb9db76",
   "metadata": {},
   "source": [
    "The MNIST data is in a slightly idiosyncratic format, we read it as follows.\n",
    "\n",
    "Note that the image pixels take values in $[0, 255]$. We know that normalization of inputs is important, so we get these back into $[0, 1]$.\n",
    "(These images contain a lot of empty space, so it is better to keep the empty space as zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e8508-418f-4ce8-ac85-8e7f1baccd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images (file_name: str) -> torch.Tensor :\n",
    "    with open(file_name, 'rb') as fp :\n",
    "        magic, numimg, numrow, numcol = np.fromfile(fp, dtype='>i4', count=4)\n",
    "        assert magic == 2051\n",
    "        return torch.from_numpy(np.fromfile(fp, dtype='>u1').astype('f4').reshape(-1, numrow*numcol)) / 255\n",
    "\n",
    "def read_labels (file_name: str) -> torch.Tensor :\n",
    "    with open(file_name, 'rb') as fp :\n",
    "        magic, numimg = np.fromfile(fp, dtype='>i4', count=2)\n",
    "        assert magic == 2049\n",
    "        return torch.from_numpy(np.fromfile(fp, dtype='>u1').astype('i8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5923761c-349d-471e-b510-3084672e68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/Users/leander/a3n_2024/Lecture_Day2_Thiele'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53ab69-f58d-462c-b9a7-ad9fe5556eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = read_images(f'{base}/train-images-idx3-ubyte')\n",
    "train_labels = read_labels(f'{base}/train-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f98e17-97cd-4b93-82cd-675d536b6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed89e2-b4cd-418a-82d4-f277a4341506",
   "metadata": {},
   "source": [
    "Let's look at these images real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f3456-9f4c-49da-a28a-c4e103694ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=8)\n",
    "for ii, a in enumerate(ax) :\n",
    "    a.matshow(train_images[ii].reshape(28, 28), cmap='Grays')\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8701b0e-42f9-4b32-bda1-7d0f61d3222f",
   "metadata": {},
   "source": [
    "We use PyTorch built-ins to construct a convenient data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfc649-e653-4689-8d44-82d29d167d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be4827-12ef-47b1-960b-0a42e23cde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_images, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4d600-d159-439b-871b-799b946bc740",
   "metadata": {},
   "source": [
    "Now we only need an optimizer and then we're ready to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931f6f5-f9ef-4f2a-a8de-082e252f21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b257807-c5a3-45b6-a45c-88e59623cda3",
   "metadata": {},
   "source": [
    "The training loop proceeds pretty much as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2901c0f-6094-4d03-958d-41606d174351",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_epochs = 32\n",
    "train_losses = []\n",
    "\n",
    "for epoch in tqdm.tqdm(range(N_epochs)) :\n",
    "    for x, label in train_loader :\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44dca47-860e-483a-a76d-9414df436d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(train_losses)\n",
    "plt.xlabel('training step')\n",
    "plt.ylabel('training loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00c11c-98f9-4d1c-92ec-06f62f62754a",
   "metadata": {},
   "source": [
    "Now, let's test our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56bb40-76cc-4d91-8b9d-78b0cf97cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = read_images(f'{base}/t10k-images.idx3-ubyte')\n",
    "test_labels = read_labels(f'{base}/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9214637e-0ca3-43ab-9764-724d5b73291e",
   "metadata": {},
   "source": [
    "The predicted most likely label can be inferred from the log-probabilities as the argmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5afe0-89ae-4106-8fdd-92d2aab06249",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad() :\n",
    "    test_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab0af0-79dc-42fb-ab2e-d4d257d71dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = test_labels == test_pred\n",
    "print(f'Prediction accurate for {correct.sum() / len(correct) * 1e2: .1f} percent of testing samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73783fe2-2070-4177-9655-7abe6aad46ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=8, nrows=8, figsize=(10, 10))\n",
    "for ii, a in enumerate(ax.flatten()) :\n",
    "    a.matshow(test_images[ii].reshape(28, 28), cmap='Grays')\n",
    "    a.text(0.05, 0.05, f'{test_pred[ii]}', transform=a.transAxes)\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd61782-6cf1-44da-b767-25fc0b3dfd14",
   "metadata": {},
   "source": [
    "### Follow-up:\n",
    "\n",
    "* How good can you make this model?\n",
    "* Which hyperparameters are most important?\n",
    "* Is the size of the training set a limitation?\\\n",
    "  Investigate by splitting off 20 percent of the training data for validation and monitoring the validation loss along the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec90d21-e870-4ab3-942b-831490c67e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
